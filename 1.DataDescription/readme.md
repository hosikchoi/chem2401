# Data Description

## 1. Data Description

Tox21 데이터와 AC50는 독성 평가 및 약물 개발 연구에서 중요한 역할을 합니다.

> 1. Tox21 데이터   
>    `Tox21` dataset은 특정 화합물에 대한 독성 예측을 위한 대규모 데이터셋으로, 미국 환경보호청(EPA), 미국 보건복지부(DHHS), 미국 식품의약국(FDA), 미국 국립보건원(NIH) 등 여러 정부 기관이 협력하여 개발했습니다. 데이터에는 여러 가지 생체 독성 반응을 나타내는 변수가 포함되어 있습니다. 변수로는 화합물 id, 화학 구조, NR 관련 변수,  SR 관련 변수가 있습니다.
> 
>    **화합물 id** 
>    - mol_id: 각 화합물의 고유 식별자
>      
>    **화학 구조**      
>    - smiles: 화합물의 화학적 구조를 나타내는 SMILES(Simplified Molecular Input Line Entry System) 문자열
>   
>    **NR (핵수용체) 관련 변수**      
>    - NR-AR: 안드로겐 수용체(Androgen Receptor)에 대한 반응 여부   
>    - NR-AR-LBD: 안드로겐 수용체의 리간드 결합 도메인에 대한 반응   
>    - NR-AhR: 아릴 하이드로카본 수용체(Aryl hydrocarbon receptor)에 대한 반응   
>    - NR-Aromatase: 아로마타제 효소에 대한 반응   
>    - NR-ER: 에스트로겐 수용체(Estrogen Receptor)에 대한 반응   
>    - NR-ER-LBD: 에스트로겐 수용체의 리간드 결합 도메인에 대한 반응   
>    - NR-PPAR-gamma: PPAR 감마 수용체에 대한 반응 (Peroxisome Proliferator-Activated Receptor Gamma)
>
>    **SR (스트레스 반응) 관련 변수**      
>    - SR-ARE: 항산화 반응 요소(Antioxidant Response Element)에 대한 반응   
>    - SR-ATAD5: DNA 손상 반응과 관련된 ATAD5에 대한 반응   
>    - SR-HSE: 열 충격 요소(Heat Shock Element)에 대한 반응   
>    - SR-MMP: 매트릭스 메탈로프로테아제(Matrix Metalloproteinase)에 대한 반응   
>    - SR-p53: 종양 억제 단백질 p53에 대한 반응
> 
>    ![image](https://github.com/user-attachments/assets/ce400220-f0ff-4473-bb81-47d57ffa051e)
>    출처: [EPA’s Distributed Structure-Searchable Toxicity (DSSTox) Database, EPA][1]   
>   
>   [1]: "https://www.epa.gov/comptox-tools/distributed-structure-searchable-toxicity-dsstox-database"



    
> 2. AC50 (Half-maximal Activity Concentration) 데이터    
>    `Tox21_ac50` dataset은 화합물의 AC50 수치를 기록한 데이터입니다. AC50는 화합물이 활성 부위에서 반응의 절반을 일으키는 화합물의 농도입니다. 약리학 및 독성학에서 매우 중요한 지표로, 특정 화합물이 생체에서 반응을 일으키기 위해 필요한 최소 농도를 평가하는 데 사용됩니다. 예를 들어, Tox21 데이터 내 화합물의 AC50 값은 화합물이 특정 독성 반응(예: 세포 사멸, 효소 억제)을 절반 정도 일으키는 데 필요한 농도를 나타냅니다. 데이터의 변수는 화합물 이름, CAS 등록 번호, 화학 구조, AC50로 이루어져 있습니다.   
>    
>    **화합물 이름**         
>    - PREFERRED NAME: 각 화합물의 고유 식별자   
>    
>    **CAS 등록 번호**       
>    - CASRN: CAS 등록 번호. 각 화합물에 대한 고유 번호로, 전 세계적으로 화합물 식별에 사용   
>      
>    **화학 구조**     
>    - SMILES: 화합물의 화학적 구조를 나타내는 SMILES(Simplified Molecular Input Line Entry System) 문자열   
>        
>    **AC50**   
>    - AC50: 화합물이 반응의 절반을 유발하는 농도 (단위: µM). AC50 값이 낮을수록 해당 화합물이 강한 독성을 나타낼 가능성이 큼
>
>    ![image](https://github.com/user-attachments/assets/f68a9734-2da9-4130-82bb-633ae76d491a)   
>    출처: [EC50, Wikipedia][1]   
>   
>    [1]: "https://en.wikipedia.org/wiki/EC50"

---
# 머신러닝 워크플로우 (Machine Learning Workflow)

## Workflow  
> 0) 데이터 수집 1) 탐색적 데이터 분석(EDA) 2) 데이터 전처리 3) 모델링 및 훈련 4) 모델 성능 평가 5) 배포
> 
![image](https://github.com/user-attachments/assets/d260ba30-1453-42b5-9ae4-b569e637ddfb)  
출처: https://wikidocs.net/31947

머신러닝은 컴퓨터가 데이터에서 패턴을 학습하여 스스로 예측하거나 의사결정을 내릴 수 있도록 돕는 기술입니다. 단순한 계산이나 규칙 기반 프로그램과 달리, 머신러닝은 데이터 기반으로 스스로 규칙을 학습하고 이를 활용합니다. 머신러닝의 성공적인 구현을 위해서는 데이터를 다루고 모델을 설계하며 평가하는 일련의 체계적인 과정이 필요합니다. 이를 머신러닝 워크플로우라고 합니다.  
머신러닝 워크플로우는 데이터 처리에서 모델 배포까지의 단계별 과정을 의미하며, 머신러닝 프로젝트를 수행하는 데 필요한 구조를 제공합니다. 각각의 단계는 서로 연결되어 있으며, 단계별 작업이 제대로 이루어져야 최종 모델이 높은 성능을 발휘할 수 있습니다. 이 워크플로우는 단순히 모델을 만드는 것을 넘어, 데이터를 이해하고 전처리하며 결과를 평가하는 모든 과정을 포함합니다.
  
본 교재에서는 수집된 데이터를 이용하기 때문에, `0)데이터 수집`, `5) 배포`를 제외한 `1)EDA`, `2)데이터 전처리`, `3)모델링`, `4)모델 성능 평가`를 위주로 설명하겠습니다.    


---
## 0) 데이터 수집  

데이터 수집은 머신러닝 워크플로우에서 가장 처음 수행하는 단계로, 학습에 필요한 데이터를 확보하는 과정입니다. 이 단계는 머신러닝 프로젝트의 성패를 결정짓는 중요한 출발점입니다. 데이터가 부족하거나 품질이 낮으면 이후 단계에서 아무리 좋은 모델을 설계하더라도 기대한 결과를 얻기 어렵습니다. 따라서 데이터 수집은 다음과 같은 요소들을 고려해 체계적으로 이루어져야 합니다. 
  
첫 번째로는 기업이나 연구 기관에서 제공하는 데이터베이스에서 데이터를 가져오는 방법이 있습니다. 예를 들면, 공공데이터 포털, Kaggle, UCI Machine Learning Repository 등이 있습니다. 화학 데이터를 위한 대표적인 데이터베이스는 ToxCast, PubChem, ChEMBL 등이 있습니다. ToxCast는 미국 환경보호청(EPA)에서 제공하는 화합물 독성 데이터를, PubChem은 화합물의 분자 구조와 생물학적 활성 데이터를, ChEMBL은 약물 화합물을 다루는 데이터베이스입니다. 이런 데이터베이스 기반 데이터들은 연구 시간과 비용을 절약할 수 있다는 장점이 있지만, 연구 목적과 완전히 일치하는 데이터를 얻지 못하는 경우도 있습니다.
두 번째로는 웹사이트나 연구 논문에서 데이터를 자동으로 수집하는 웹크롤링 방법입니다. Python과 같은 프로그래밍 언어를 사용해 크롤링 스크립트를 작성할 수 있습니다. 이 방식을 사용할 때에는 웹사이트 이용 약관을 준수하고, 수집한 데이터를 합법적으로 사용하는지 확인하여야 합니다.
이 외에도 설문조사 및 실험데이터, 센서 데이터, 데이터 증강 등 여러 데이터 수집 과정이 존재합니다. 본 교재에서는 첫번째 방법인 미국 환경보호청(EPA)에서 제공하는 데이터 Tox21을 이용하였습니다.
  
  
---  
## 1) 탐색적 데이터 분석 (Exploratory Data Analysis, EDA)  

데이터가 수집되었다면, 데이터를 점검하고 탐색하는 단계입니다. 데이터의 구조, 노이즈 데이터, 머신 러닝 적용을 위해서 데이터를 어떻게 정재해야하는지 등을 파악해야 합니다. 이 단계를 **탐색적 데이터 분석(Exploratory Data Analysis, EDA)** 라고 합니다. 이는 독립 및 종속 변수, 변수의 분포, 이상치, 요약통계량 등을 점검하며 데이터의 특징과 내재하는 구조적 관계를 알아내는 과정으로 이 과정에서 시각화(ex. 히스토그램, box plot, 산점도)와 간단한 통계 검정(ex. 정규성 검정, 평균차이 검정, 상관관계 검정)을 진행하기도 합니다. EDA는 최종적으로는 모델의 정확성을 높이기 위한 전 단계 작업입니다. 그렇기 때문에 EDA 과정은 데이터 구조를 파악해 그에 맞는 처리 방법을 결정하는데 중요한 역할을 합니다.   

**EDA의 주요 목적**
- 데이터 이해: 데이터의 구조, 변수의 특성, 분포 등을 파악하여 데이터셋의 전반적인 모습을 이해합니다.
- 데이터 품질 평가: 결측치, 이상치, 중복 데이터 등을 식별하여 데이터의 신뢰성을 평가하고, 필요한 전처리 과정을 계획합니다.
- 패턴 및 관계 탐색: 변수 간의 상관관계나 잠재적인 패턴을 발견하여 데이터의 내재된 특성을 파악합니다.
- 가설 설정 및 검증: 데이터를 기반으로 초기 가설을 설정하고, 이를 검증하기 위한 방향성을 도출합니다.
   
**EDA에서 활용되는 기법**
- 기술 통계 분석: 평균, 중앙값, 분산, 표준편차 등 기본적인 통계량을 계산하여 데이터의 중심 경향성과 분포를 파악합니다.
- 데이터 시각화: 히스토그램, 박스플롯, 산점도, 상관 행렬 등 다양한 시각화 도구를 활용하여 데이터의 분포와 변수 간 관계를 직관적으로 이해합니다.
- 상관 분석: 변수들 간의 상관계수를 계산하여 변수 간의 선형 관계의 강도와 방향을 평가합니다.
결측치 및 이상치 탐지: 데이터 내의 결측치와 이상치를 식별하고, 이를 처리하기 위한 전략을 수립합니다.

**EDA의 일반적인 절차**
- 데이터 구조 파악: 데이터의 크기, 변수의 수, 변수의 데이터 타입 등을 확인하여 데이터셋의 기본 구조를 이해합니다.
- 데이터 요약 및 시각화: 기술 통계량을 계산하고, 다양한 시각화 기법을 통해 데이터의 분포와 특성을 파악합니다.
- 결측치 및 이상치 처리: 결측치와 이상치를 식별하고, 분석에 미치는 영향을 최소화하기 위한 적절한 처리를 수행합니다.
- 변수 간 관계 분석: 상관 분석과 시각화를 통해 변수들 간의 관계를 탐색하고, 중요한 변수들을 식별합니다.
- 가설 설정 및 검증: 데이터 분석을 통해 도출된 인사이트를 기반으로 가설을 설정하고, 이를 검증하기 위한 추가 분석 계획을 수립합니다.


  
본 교재는 tox21 데이터셋과 tox21_ac50 데이터셋을 통해 데이터 전처리와 eda 과정을 거쳤습니다. 다음은 데이터 전처리와 eda를 수행한 코드를 나타낸 것입니다.
> 1-1. tox21_EDA  
> 범주형 데이터를 가지는 전처리 전 데이터셋 `tox21.xlsm`을 EDA한 코드입니다. 코드를 통해 결측값을 확인할 수 있습니다. 

> 1-2. tox21_ac50_EDA  
> `tox21.xlsm`을 보충하기 위해, 연속형 데이터를 가지는 데이터셋 `tox21_ac50.xlsx`을 EDA한 코드입니다. 코드를 통해 결측값을 제거한 후 요약통계량을 확인하고, 시각화를 할 수 있습니다.

> 1-3. tox21_Preprocessing  
> `tox21.xlsm`의 데이터 전처리 코드입니다.
> - 이용 패키지: `pandas`, `numpy`, `rdkit`
> - `rdkit` 패키지는 화학 데이터 처리에서 이용됩니다. 해당 노트에서는 `rdkit.Chem`을 통해 SMILES 문자열을 분자 객체로 변환하고, `rdkit.Chem.MACCSkeys`를 통해 분자의 특징을 이진 형식으로 표현한 MACCS 지문을 생성하는데 사용됩니다.

**본 교재에서는 EDA 과정에서 시각화를 메인으로 합니다.**

  
![image](https://github.com/user-attachments/assets/b004e8de-366b-419f-bf86-86087c9b255d)   
출처: seaborn 시각화 예제(https://kite-mo.github.io/python/2020/04/20/python-5-weeks/)

![image](https://github.com/user-attachments/assets/7f8a274e-bde7-4887-a472-ab3c9c87e908)   
출처: 평균 차이 검정 중 분산 분석(ANOVA) (https://recipesds.tistory.com/entry/%EB%B6%84%EC%82%B0%EB%B6%84%EC%84%9DANOVA-%EC%95%84%EB%8B%88-%EB%8B%A4%EC%A7%91%EB%8B%A8-%EC%B0%A8%EC%9D%B4%EC%97%90-%EC%99%9C-%EA%B0%91%EC%9E%90%EA%B8%B0-%EB%B6%84%EC%82%B0%EC%9D%84-%EB%B3%B4%EC%A7%80%EC%9A%94)

![image](https://github.com/user-attachments/assets/e831c2a6-d3a8-4f47-a04e-f90260780f18)   
출처: 피어슨 상관계수 (https://statisticsplaybook.com/covariance-and-correlation/)   


---

## 2) 데이터 전처리

데이터 전처리는 머신러닝 모델의 성능을 높이고 데이터의 품질을 향상시키기 위한 중요한 과정입니다. 데이터 전처리 과정에서는 필요에 따라서 수행하는 내용이 달라질 수 있습니다. 주로 결측치 처리, 데이터 스케일링, 인코딩, 이상치 처리, 특성 엔지니어링, 데이터 분리, 데이터 증강 등과 같은 다양한 방법을 수행합니다.   

1. 결측치 처리
> 결측치를 무시하면 데이터 품질이 저하되고, 모델 성능이 나빠지며, 결과 해석에도 오류가 발생합니다. 따라서 적절한 방법(삭제, 대체, 예측 모델 등)을 통해 결측치를 처리하는 것이 중요합니다. 또한 결측치가 무작위로 발생했는지, 패턴이 있는지 확인하기 위해 Missingno 라이브러리를 사용해 결측치 히트맵을 시각화할 수 있습니다.
>
> - 삭제: 결측값이 많지 않은 경우, 결측값이 있는 행이나 열을 삭제할 수 있습니다. 데이터 손실에 유의해야 합니다.   
> - 대체: 결측치를 평균, 중앙값, 최빈값 등으로 대체하거나, KNN Imputation와 같은 기법을 통해 예측하여 채웁니다. 이는 데이터의 일관성을 유지하는 데 유용합니다.   
> - 예측 모델: 머신러닝 모델을 사용해 결측치를 예측하고 채울 수 있습니다. 이는 특히 결측 패턴이 복잡할 때 유용합니다. RandomForest Regressor 등이 그 예시입니다.
   
2. 데이터 스케일링  
> - 표준화(Standardization): 데이터의 평균을 0, 표준편차를 1로 맞추는 방법입니다. 특히, SVM, KNN, K-Means 같은 거리 기반 모델에 효과적입니다.  
> - 정규화(Normalization): 데이터를 0과 1 사이로 스케일링하여 변환합니다. 일반적으로 최소값을 0, 최대값을 1로 설정하는 Min-Max 스케일링을 사용합니다. 정규화는 신경망 모델에 적합합니다.  
> - 로버스트 스케일링(Robust Scaling): 이상치에 민감하지 않도록 중앙값과 사분위 범위를 사용하여 스케일링합니다. 이상치가 많을 때 유용합니다.

3. 인코딩   
> - 레이블 인코딩(Label Encoding): 범주형 변수를 정수로 변환합니다. 예를 들어, ‘남성’을 0, ‘여성’을 1로 변환할 수 있습니다. 그러나 레이블 간 순서가 없을 때는 부적절할 수 있습니다.   
> - 원-핫 인코딩(One-Hot Encoding): 각 카테고리를 이진 벡터로 변환하여 순서의 의미를 제거합니다. 예를 들어, ‘빨강’, ‘파랑’, ‘초록’이라는 카테고리를 [1, 0, 0], [0, 1, 0], [0, 0, 1]과 같이 변환합니다.
> - 임베딩(Embedding): 주로 자연어 처리에서 사용하며, 고차원 벡터로 카테고리를 변환해 의미와 패턴을 보존합니다. 인공신경망에 적합합니다.

4. 이상치 처리   
> - 이상치 탐지 및 제거: 이상치는 데이터 분포를 왜곡할 수 있으므로 박스 플롯, IQR, Z-점수 등을 사용하여 이상치를 탐지하고 제거합니다.    
> - 대체: 극단값을 평균이나 중앙값 등의 특정 값으로 대체합니다.   
> - 변환(Transformation): 로그 변환, 제곱근 변환 등을 적용하여 이상치의 영향을 줄일 수 있습니다.     

5. 특성 엔지니어링(Feature Engineering)   
> - 특성 생성: 기존 데이터를 이용해 새로운 특성을 생성합니다. 예를 들어, 날짜 데이터를 기반으로 ‘연도’, ‘월’, ‘일’ 등의 특성을 추출할 수 있습니다.   
> - 차원 축소: PCA, LDA 등 차원 축소 기법을 사용해 불필요한 특성을 제거하고 모델의 효율성을 높입니다.   
> - 다항식 특성(Polynomial Features): 특정 변수의 상호작용을 반영하기 위해 변수 간의 곱이나 제곱을 생성하여 추가합니다. 예를 들어, x와 y라는 특성의 제곱과 곱을 생성하여 모델에 반영할 수 있습니다.

6. 데이터 분리(Data Splitting)   
> - 훈련/검증/테스트 세트 분리: 데이터를 훈련, 검증, 테스트 세트로 나눠서 모델의 성능을 평가합니다. 일반적으로 70:15:15 또는 80:10:10 비율로 나눕니다.   
> - 교차 검증(Cross-Validation): 데이터를 여러 번 훈련과 테스트로 나누어 모델의 일반화 성능을 평가하는 방법입니다. K-교차 검증(K-fold Cross-Validation)이 가장 많이 사용됩니다.

7. 데이터 증강(Data Augmentation)
> 
> - 이미지 데이터 증강: 회전, 크기 조정, 플립 등 다양한 변형을 통해 데이터 다양성을 높여 모델이 다양한 패턴을 학습할 수 있도록 합니다.    
> - 텍스트 데이터 증강: 문장 내 단어 순서 변경, 동의어 교체 등을 통해 텍스트 데이터를 증가시키는 방법입니다.    
> - 시간 시계열 데이터 증강: 시계열 데이터의 일부 구간을 샘플링하거나 노이즈를 추가하는 방법이 있습니다.


해당 단원에서 더욱 자세하게 설명하겠습니다.
---

## 3) 모델링 및 훈련 (Model Fitting & Training)



**본 교재는 아래의 10가지 모델을 통해 독성 유무 예측합니다.**


* 3-1.Logistic_Regression


* 3-2.Decision_Tree_Classifier


* 3-3.Random_Forest_Classifier 


* 3-4.Gradient_Boosting_Classifier 


* 3-5.XGB_Classifier


* 3-6.LGBM_Classifier


* 3-7.Linear_Discriminant_Analysis


* 3-8.Quadratic_Discriminant_Analysis


* 3-9.PLS_Regression


* 3-10.MLP_Classifier

---
## 4. 모델 성능 평가(Model Evaluation)

평가지표와 교차 검증을 통해 모델 성능 평가하는 단계입니다.

### 4-1. Evaluation

모델을 구축하는 과정은 Training Data를 통해 모델에 Fitting 한 후, Test Data로 테스트하는 과정을 거칩니다.
실제 값과 예측 값의 차이를 의미하는 Error가 작은 Fit 한 모델을 만드는 것이 목적입니다.

1. 분류모델
- Confusion Matrix (Accuracy, Precision, Sensitivity, Specificity)
- F1 Score
- ROC AUC Curve
    
2. 회귀모델
- MSE
- MAPE
- R2 score

### Cross-Validation이란?
교차 검증(Cross-Validation)은 모델이 데이터를 얼마나 잘 일반화할 수 있는지를 평가하기 위해 데이터셋을 여러 부분으로 나눠 여러 번 학습-평가 과정을 반복하는 기법입니다. 과적합을 방지, 데이터 활용의 극대화, 성능의 안정성 평가, 모델 선택에 도움을 주기 위해 이용합니다.
- K-Fold 
- Stratified K-Fold
