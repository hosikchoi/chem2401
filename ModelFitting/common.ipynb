{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from itertools import product\n",
    "from collections.abc import Iterable\n",
    "\n",
    "from sklearn.model_selection import (\n",
    "    StratifiedKFold, \n",
    "    StratifiedShuffleSplit\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    accuracy_score,\n",
    "    roc_auc_score,\n",
    "    f1_score\n",
    ")\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data_split 함수는 데이터를 분할하는 함수입니다. scikit-learn의 StratifiedShuffleSplit을 사용해 데이터를 분할합니다. \n",
    "- StratifiedShuffleSplit은 데이터를 섞은 후에 나누는 것이 아니라, 데이터를 나눈 후에 섞는 방식으로 동작합니다. \n",
    "\n",
    "\n",
    "함수는 아래와 같은 과정으로 동작합니다.\n",
    "1. StratifiedShuffleSplit의 객체, sss를 생성합니다.\n",
    "- sss는 데이터를 1번 분할하고, 테스트 데이터의 비율을 20%로 설정한다는 정보가 담겨있습니다.\n",
    "\n",
    "2. sss.split(X, y)를 호출합니다.\n",
    "- X는 데이터의 feature, y는 데이터의 label입니다.\n",
    "- split 함수는 X와 y를 test_size 비율로 나눈 후에, train_idx와 test_idx를 반환합니다.\n",
    "- train_idx와 test_idx는 각각 train 데이터와 test 데이터에 사용할 데이터의 인덱스입니다.\n",
    "\n",
    "3. train_idx와 test_idx를 사용해 train_x, test_x, train_y, test_y에 해당되는 데이터를 저장합니다.\n",
    "- reset_index(drop = True)는 인덱스를 0부터 다시 시작하도록 설정함을 의미합니다.\n",
    "\n",
    "4. train_x, test_x, train_y, test_y를 반환합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_split(X, y, seed):\n",
    "    sss = StratifiedShuffleSplit(n_splits = 1, test_size = 0.2, random_state = seed)\n",
    "    \n",
    "    for train_idx, test_idx in sss.split(X, y):\n",
    "        train_x = X.iloc[train_idx].reset_index(drop = True)\n",
    "        train_y = y.iloc[train_idx].reset_index(drop = True)\n",
    "        test_x = X.iloc[test_idx].reset_index(drop = True)\n",
    "        test_y = y.iloc[test_idx].reset_index(drop = True)\n",
    "    \n",
    "    return train_x, test_x, train_y, test_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "parameter_grid는 하이퍼파라미터의 dictionary를 받아서 가능한 모든 조합의 하이퍼파라미터의 리스트를 생성하고 이를 반환합니다.\n",
    "\n",
    "\n",
    "함수의 동작 과정은 아래와 같습니다\n",
    "1. 들어온 입력이 dictionary가 아니면 TypeError를 발생시킵니다.\n",
    "2. dictionary의 value가 iterable이 아니면 TypeError를 발생시킵니다.\n",
    "    - iterable이란 list, tuple, set, dict, str 와 같이 맴버들을 한 번에 한 개씩 반환할 수 있는 객체를 말합니다.\n",
    "    - int, float, bool 등의 객체는 iterable 하지 않습니다.\n",
    "\n",
    "3. dictionary의 항목들을 key를 기준으로 정렬합니다.\n",
    "4. dictionary의 key와 value를 각각 keys와 values에 저장합니다.\n",
    "5. itertools.product를 사용하여 values의 모든 조합을 생성합니다.\n",
    "6. 생성된 조합을 dictionary로 변환하여 params_grid에 저장합니다.\n",
    "7. params_grid를 반환합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parameter_grid(param_dict):\n",
    "    if not isinstance(param_dict, dict):\n",
    "        raise TypeError('Parameter grid is not a dict ({!r})'.format(param_dict))\n",
    "    \n",
    "    if isinstance(param_dict, dict):\n",
    "        for key in param_dict:\n",
    "            if not isinstance(param_dict[key], Iterable):\n",
    "                raise TypeError('Parameter grid value is not iterable '\n",
    "                                '(key={!r}, value={!r})'.format(key, param_dict[key]))\n",
    "    \n",
    "    items = sorted(param_dict.items())\n",
    "    keys, values = zip(*items)\n",
    "    \n",
    "    params_grid = []\n",
    "    for v in product(*values):\n",
    "        params_grid.append(dict(zip(keys, v))) \n",
    "    \n",
    "    return params_grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "multiclass_cross_validation은 모델과 데이터를 받아서 5-fold cross validation을 수행하고, 각 fold에 대한 평균 precision, recall, f1, accuracy를 계산하여 반환하는 함수입니다.\n",
    "\n",
    "k-fold cross validation을 수행하기 위해서 StratifiedKFold를 사용합니다. \n",
    "- StratifiedKFold는 각 fold에 대해서 클래스의 비율이 유지되도록 데이터를 분할합니다. \n",
    "\n",
    "평가를 수행하는 metric은 아래와 같습니다.\n",
    "1. precision\n",
    "2. recall\n",
    "3. f1\n",
    "4. accuracy\n",
    "\n",
    "각 fold에 대한 평가를 저장하기 위한 변수(리스트)를 선언합니다.\n",
    "\n",
    "\n",
    "skf.split(x, y)는 x와 y를 5-fold로 분할한 후, 각 fold에 대한 train index와 validation index를 반환하는 함수입니다.\n",
    "해당되는 index를 사용하여 train_x, train_y, val_x, val_y를 추출하고, 모델을 학습시킵니다.\n",
    "\n",
    "\n",
    "PLSRegression은 다중 클래스 분류를 지원하지 않기 때문에, 별도로 one-hot encoding을 수행합니다.\n",
    "- 예측값을 처리할 때, argmax를 사용하여 가장 높은 확률을 가진 클래스를 선택합니다.\n",
    "\n",
    "\n",
    "학습된 모델을 사용하여 train_x, val_x에 대한 예측값을 구하고, 각 metric을 계산합니다.\n",
    "- metric을 계산할 때, average = 'macro'로 설정합니다.\n",
    "    - average = 'macro'는 클래스별로 metric을 계산한 후, 평균을 계산하는 방식입니다.\n",
    "\n",
    "\n",
    "반환될 result는 다음과 형태를 가집니다.\n",
    "\n",
    "{\n",
    "    'train_precision': 0.95,\n",
    "    'train_recall': 0.94,\n",
    "    'train_f1': 0.94,\n",
    "    'train_accuracy': 0.95,\n",
    "    'val_precision': 0.92,\n",
    "    'val_recall': 0.91,\n",
    "    'val_f1': 0.91,\n",
    "    'val_accuracy': 0.92\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiclass_cross_validation(model, x, y, seed):\n",
    "    skf = StratifiedKFold(n_splits = 5, shuffle = True, random_state = seed)\n",
    "    \n",
    "    metrics = ['precision', 'recall', 'f1', 'accuracy']\n",
    "    \n",
    "    train_metrics = list(map(lambda x: 'train_' + x, metrics))\n",
    "    val_metrics = list(map(lambda x: 'val_' + x, metrics))\n",
    "    \n",
    "    train_precision = []\n",
    "    train_recall = []\n",
    "    train_f1 = []\n",
    "    train_accuracy = []\n",
    "    \n",
    "    val_precision = []\n",
    "    val_recall = []\n",
    "    val_f1 = []\n",
    "    val_accuracy = []\n",
    "    \n",
    "    for train_idx, val_idx in skf.split(x, y):\n",
    "        train_x, train_y = x.iloc[train_idx], y.iloc[train_idx]\n",
    "        val_x, val_y = x.iloc[val_idx], y.iloc[val_idx]\n",
    "        \n",
    "        if type(model) == sklearn.cross_decomposition._pls.PLSRegression:\n",
    "            onehot_train_y = pd.get_dummies(train_y)\n",
    "            \n",
    "            model.fit(train_x, onehot_train_y)\n",
    "            \n",
    "            train_pred = np.argmax(model.predict(train_x), axis = 1)\n",
    "            val_pred = np.argmax(model.predict(val_x), axis = 1)\n",
    "            \n",
    "        else:\n",
    "            model.fit(train_x, train_y)\n",
    "            \n",
    "            train_pred = model.predict(train_x)\n",
    "            val_pred = model.predict(val_x)\n",
    "        \n",
    "        train_precision.append(precision_score(train_y, train_pred, average = 'macro'))\n",
    "        train_recall.append(recall_score(train_y, train_pred, average = 'macro'))\n",
    "        train_f1.append(f1_score(train_y, train_pred, average = 'macro'))\n",
    "        train_accuracy.append(accuracy_score(train_y, train_pred))\n",
    "\n",
    "        val_precision.append(precision_score(val_y, val_pred, average = 'macro'))\n",
    "        val_recall.append(recall_score(val_y, val_pred, average = 'macro'))\n",
    "        val_f1.append(f1_score(val_y, val_pred, average = 'macro'))\n",
    "        val_accuracy.append(accuracy_score(val_y, val_pred))\n",
    "        \n",
    "    result = dict(zip(train_metrics + val_metrics, \n",
    "                      [np.mean(train_precision), np.mean(train_recall), np.mean(train_f1), np.mean(train_accuracy), \n",
    "                       np.mean(val_precision), np.mean(val_recall), np.mean(val_f1), np.mean(val_accuracy)]))\n",
    "    \n",
    "    return(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_cross_validation(model, x, y, seed):\n",
    "    skf = StratifiedKFold(n_splits = 5, shuffle = True, random_state = seed)\n",
    "    \n",
    "    metrics = ['precision', 'recall', 'f1', 'accuracy', 'auc']\n",
    "    \n",
    "    train_metrics = list(map(lambda x: 'train_' + x, metrics))\n",
    "    val_metrics = list(map(lambda x: 'val_' + x, metrics))\n",
    "    \n",
    "    train_precision = []\n",
    "    train_recall = []\n",
    "    train_f1 = []\n",
    "    train_accuracy = []\n",
    "    train_auc = []\n",
    "    \n",
    "    val_precision = []\n",
    "    val_recall = []\n",
    "    val_f1 = []\n",
    "    val_accuracy = []\n",
    "    val_auc = []\n",
    "    \n",
    "    for train_idx, val_idx in skf.split(x, y):\n",
    "        train_x, train_y = x.iloc[train_idx], y.iloc[train_idx]\n",
    "        val_x, val_y = x.iloc[val_idx], y.iloc[val_idx]\n",
    "        \n",
    "        if type(model) == sklearn.cross_decomposition._pls.PLSRegression:\n",
    "            model.fit(train_x, train_y)\n",
    "            \n",
    "            train_pred_score = model.predict(train_x)\n",
    "            train_pred = np.where(train_pred_score < 0.5, 0, 1).reshape(-1)\n",
    "            \n",
    "            val_pred_score = model.predict(val_x)\n",
    "            val_pred = np.where(val_pred_score < 0.5, 0, 1).reshape(-1)\n",
    "            \n",
    "        else:\n",
    "            model.fit(train_x, train_y)\n",
    "            \n",
    "            train_pred = model.predict(train_x)\n",
    "            train_pred_score = model.predict_proba(train_x)[:, 1]\n",
    "            \n",
    "            val_pred = model.predict(val_x)\n",
    "            val_pred_score = model.predict_proba(val_x)[:, 1]\n",
    "        \n",
    "        train_precision.append(precision_score(train_y, train_pred))\n",
    "        train_recall.append(recall_score(train_y, train_pred))\n",
    "        train_f1.append(f1_score(train_y, train_pred))\n",
    "        train_accuracy.append(accuracy_score(train_y, train_pred))\n",
    "        train_auc.append(roc_auc_score(train_y, train_pred_score))\n",
    "\n",
    "        val_precision.append(precision_score(val_y, val_pred))\n",
    "        val_recall.append(recall_score(val_y, val_pred))\n",
    "        val_f1.append(f1_score(val_y, val_pred))\n",
    "        val_accuracy.append(accuracy_score(val_y, val_pred))\n",
    "        val_auc.append(roc_auc_score(val_y, val_pred_score))\n",
    "\n",
    "    result = dict(zip(train_metrics + val_metrics, \n",
    "                      [np.mean(train_precision), np.mean(train_recall), np.mean(train_f1), np.mean(train_accuracy), np.mean(train_auc), \n",
    "                       np.mean(val_precision), np.mean(val_recall), np.mean(val_f1), np.mean(val_accuracy), np.mean(val_auc)]))\n",
    "    \n",
    "    return(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_smote_cross_validation(model, x, y, seed, args):\n",
    "    skf = StratifiedKFold(n_splits = 5, shuffle = True, random_state = seed)\n",
    "    \n",
    "    metrics = ['precision', 'recall', 'f1', 'accuracy', 'auc']\n",
    "    \n",
    "    train_metrics = list(map(lambda x: 'train_' + x, metrics))\n",
    "    val_metrics = list(map(lambda x: 'val_' + x, metrics))\n",
    "    \n",
    "    train_precision = []\n",
    "    train_recall = []\n",
    "    train_f1 = []\n",
    "    train_accuracy = []\n",
    "    train_auc = []\n",
    "    \n",
    "    val_precision = []\n",
    "    val_recall = []\n",
    "    val_f1 = []\n",
    "    val_accuracy = []\n",
    "    val_auc = []\n",
    "    \n",
    "    for train_idx, val_idx in skf.split(x, y):\n",
    "        train_x, train_y = x.iloc[train_idx], y.iloc[train_idx]\n",
    "        val_x, val_y = x.iloc[val_idx], y.iloc[val_idx]\n",
    "        \n",
    "        smote = SMOTE(random_state = args.smoteseed, k_neighbors = args.neighbor)\n",
    "        train_x, train_y = smote.fit_resample(train_x, train_y)\n",
    "        \n",
    "        if type(model) == sklearn.cross_decomposition._pls.PLSRegression:\n",
    "            model.fit(train_x, train_y)\n",
    "            \n",
    "            train_pred_score = model.predict(train_x)\n",
    "            train_pred = np.where(train_pred_score < 0.5, 0, 1).reshape(-1)\n",
    "            \n",
    "            val_pred_score = model.predict(val_x)\n",
    "            val_pred = np.where(val_pred_score < 0.5, 0, 1).reshape(-1)\n",
    "            \n",
    "        else:\n",
    "            model.fit(train_x, train_y)\n",
    "            \n",
    "            train_pred = model.predict(train_x)\n",
    "            train_pred_score = model.predict_proba(train_x)[:, 1]\n",
    "            \n",
    "            val_pred = model.predict(val_x)\n",
    "            val_pred_score = model.predict_proba(val_x)[:, 1]\n",
    "        \n",
    "        train_precision.append(precision_score(train_y, train_pred))\n",
    "        train_recall.append(recall_score(train_y, train_pred))\n",
    "        train_f1.append(f1_score(train_y, train_pred))\n",
    "        train_accuracy.append(accuracy_score(train_y, train_pred))\n",
    "        train_auc.append(roc_auc_score(train_y, train_pred_score))\n",
    "\n",
    "        val_precision.append(precision_score(val_y, val_pred))\n",
    "        val_recall.append(recall_score(val_y, val_pred))\n",
    "        val_f1.append(f1_score(val_y, val_pred))\n",
    "        val_accuracy.append(accuracy_score(val_y, val_pred))\n",
    "        val_auc.append(roc_auc_score(val_y, val_pred_score))\n",
    "\n",
    "    result = dict(zip(train_metrics + val_metrics, \n",
    "                      [np.mean(train_precision), np.mean(train_recall), np.mean(train_f1), np.mean(train_accuracy), np.mean(train_auc), \n",
    "                       np.mean(val_precision), np.mean(val_recall), np.mean(val_f1), np.mean(val_accuracy), np.mean(val_auc)]))\n",
    "    \n",
    "    return(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "사용되지 않는 함수입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def CV(x, y, model, params, seed):\n",
    "#     skf = StratifiedKFold(n_splits = 5, shuffle = True, random_state = seed)\n",
    "    \n",
    "#     metrics = ['precision', 'recall', 'f1', 'accuracy']\n",
    "    \n",
    "#     train_metrics = list(map(lambda x: 'train_' + x, metrics))\n",
    "#     val_metrics = list(map(lambda x: 'val_' + x, metrics))\n",
    "    \n",
    "#     train_precision = []\n",
    "#     train_recall = []\n",
    "#     train_f1 = []\n",
    "#     train_accuracy = []\n",
    "    \n",
    "#     val_precision = []\n",
    "#     val_recall = []\n",
    "#     val_f1 = []\n",
    "#     val_accuracy = []\n",
    "    \n",
    "#     for train_idx, val_idx in skf.split(x, y):\n",
    "#         train_x, train_y = x.iloc[train_idx], y.iloc[train_idx]\n",
    "#         val_x, val_y = x.iloc[val_idx], y.iloc[val_idx]\n",
    "        \n",
    "#         try:\n",
    "#             clf = model(random_state = seed, **params)\n",
    "#         except:\n",
    "#             clf = model(**params)\n",
    "        \n",
    "        \n",
    "#         if model == sklearn.cross_decomposition._pls.PLSRegression:\n",
    "#             onehot_train_y = pd.get_dummies(train_y)\n",
    "            \n",
    "#             clf.fit(train_x, onehot_train_y)\n",
    "            \n",
    "#             train_pred = np.argmax(clf.predict(train_x), axis = 1)\n",
    "#             val_pred = np.argmax(clf.predict(val_x), axis = 1)\n",
    "            \n",
    "#         else:\n",
    "#             clf.fit(train_x, train_y)\n",
    "            \n",
    "#             train_pred = clf.predict(train_x)\n",
    "#             val_pred = clf.predict(val_x)\n",
    "        \n",
    "#         train_precision.append(precision_score(train_y, train_pred, average = 'macro'))\n",
    "#         train_recall.append(recall_score(train_y, train_pred, average = 'macro'))\n",
    "#         train_f1.append(f1_score(train_y, train_pred, average = 'macro'))\n",
    "#         train_accuracy.append(accuracy_score(train_y, train_pred))\n",
    "\n",
    "#         val_precision.append(precision_score(val_y, val_pred, average = 'macro'))\n",
    "#         val_recall.append(recall_score(val_y, val_pred, average = 'macro'))\n",
    "#         val_f1.append(f1_score(val_y, val_pred, average = 'macro'))\n",
    "#         val_accuracy.append(accuracy_score(val_y, val_pred))\n",
    "        \n",
    "#     result = dict(zip(['params'] + train_metrics + val_metrics, \n",
    "#                       [params] + [np.mean(train_precision), \n",
    "#                                   np.mean(train_recall), \n",
    "#                                   np.mean(train_f1), \n",
    "#                                   np.mean(train_accuracy), \n",
    "#                                   np.mean(val_precision), \n",
    "#                                   np.mean(val_recall), \n",
    "#                                   np.mean(val_f1), \n",
    "#                                   np.mean(val_accuracy)]))\n",
    "    \n",
    "#     return(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric_mean(data, metric: str):\n",
    "    mean_per_hp = list(map(lambda x: np.mean(x[1]), data[metric].items()))\n",
    "    return mean_per_hp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print_best_param은 val_result에서 metric에 해당하는 평균값이 가장 높은 파라미터를 반환하는 함수입니다.\n",
    "\n",
    "\n",
    "함수는 아래의 과정으로 동작합니다.\n",
    "1. val_result에서 metric에 해당하는 평균값을 구합니다.\n",
    "2. 평균값이 가장 높은 파라미터의 인덱스를 구합니다.\n",
    "3. val_result에서 해당 인덱스에 해당하는 파라미터를 반환합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_best_param(val_result, metric: str):\n",
    "    \n",
    "    mean_list = metric_mean(val_result, metric)\n",
    "    max_idx = mean_list.index(max(mean_list))\n",
    "    \n",
    "    best_param = val_result['model'][f'model{max_idx}']\n",
    "    \n",
    "    return best_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_val_result(path: str, tg_num: int, inhale_type: str, model: str, is_smote = True):\n",
    "    if is_smote:\n",
    "        try:\n",
    "            with open(f'{path}/tg{tg_num}_val_results/binary_smote5/{inhale_type}_{model}.json', 'r') as file:\n",
    "                val_result = json.load(file)\n",
    "        except:\n",
    "            with open(f'{path}tg{tg_num}_val_results/binary_smote5/{inhale_type}_{model}.json', 'r') as file:\n",
    "                val_result = json.load(file)\n",
    "    else:\n",
    "        try:\n",
    "            with open(f'{path}/tg{tg_num}_val_results/binary/{inhale_type}_{model}.json', 'r') as file:\n",
    "                val_result = json.load(file)\n",
    "        except:\n",
    "            with open(f'{path}tg{tg_num}_val_results/binary/{inhale_type}_{model}.json', 'r') as file:\n",
    "                val_result = json.load(file)\n",
    "    \n",
    "    return val_result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "사용되지 않는 함수입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def print_best_param(path: str, tg_num: int, inhale_type: str, model: str, metric: str):\n",
    "#     data = load_val_result(path, tg_num, inhale_type, model)\n",
    "    \n",
    "#     mean_list = metric_mean(data, 'f1')\n",
    "#     max_idx = mean_list.index(max(mean_list))\n",
    "    \n",
    "#     best_param = data['model'][f'model{max_idx}']\n",
    "    \n",
    "#     return best_param"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
