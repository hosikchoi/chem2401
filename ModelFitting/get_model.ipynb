{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scikit-learn은 파이썬에서 머신러닝을 구현할 때 사용하는 라이브러리입니다.\n",
    "\n",
    "scikit-learn에서 제공하는 머신러닝 알고리즘을 가져와 사용합니다.\n",
    "- xgboost와 lightgbm은 scikit-learn에서 제공하지 않습니다.\n",
    "1. Logistic Regression\n",
    "2. PLSRegression\n",
    "3. LinearDiscriminantAnalysis\n",
    "4. QuadraticDiscriminantAnalysis\n",
    "5. DecisionTreeClassifier\n",
    "6. RandomForestClassifier\n",
    "7. GradientBoostingClassifier\n",
    "8. xgboost\n",
    "9. lightgbm\n",
    "10. MLPClassifier\n",
    "\n",
    "\n",
    "parameter_grid(common 항목 참조)는 scikit-learn에서 제공하는 머신러닝 알고리즘의 하이퍼파라미터를 설정하는 파이썬 파일입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.discriminant_analysis import (\n",
    "    LinearDiscriminantAnalysis, \n",
    "    QuadraticDiscriminantAnalysis\n",
    ")\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier, \n",
    "    GradientBoostingClassifier\n",
    ")\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from module.common import parameter_grid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load_model은 모델명과 파라미터를 받아서 모델을 생성하는 함수입니다. 모델명은 문자열로 받고, 파라미터는 딕셔너리로 받습니다. 파라미터는 모델마다 다르기 때문에 딕셔너리로 받아서 모델에 넣어줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model: str, seed: int, param: dict):\n",
    "    if model == 'logistic':\n",
    "        clf = LogisticRegression(random_state = seed, **param)\n",
    "        \n",
    "    elif model == 'dt':\n",
    "        clf = DecisionTreeClassifier(random_state = seed, **param)\n",
    "    \n",
    "    elif model == 'rf':\n",
    "        clf = RandomForestClassifier(random_state = seed, **param)\n",
    "    \n",
    "    elif model == 'gbt':\n",
    "        clf = GradientBoostingClassifier(random_state = seed, **param)\n",
    "    \n",
    "    elif model == 'xgb':\n",
    "        clf = XGBClassifier(random_state = seed, **param)\n",
    "    \n",
    "    elif model == 'lgb':\n",
    "        clf = LGBMClassifier(random_state = seed, **param)\n",
    "    \n",
    "    elif model == 'lda':\n",
    "        clf = LinearDiscriminantAnalysis(**param)\n",
    "    \n",
    "    elif model == 'qda':\n",
    "        clf = QuadraticDiscriminantAnalysis(**param)\n",
    "    \n",
    "    elif model == 'plsda':\n",
    "        clf = PLSRegression(**param)\n",
    "        \n",
    "    elif model == 'mlp':\n",
    "        clf = MLPClassifier(random_state = seed, **param)\n",
    "    \n",
    "    return clf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load_hyperparameter는 모델마다 하이퍼파라미터를 설정해주는 함수입니다. 모델의 이름을 문자열로 받아 해당되는 모델에 적합한 하이퍼파라미터를 반환합니다.\n",
    "\n",
    "lda의 경우 solver에 따라서 하이퍼파라미터가 달라지기 때문에 두 가지 경우를 모두 고려해줍니다.\n",
    "1. lsqr과 eigen의 경우 shrinkage값을 포함합니다.\n",
    "2. svd의 경우 tol값을 포함합니다.\n",
    "\n",
    "dictionary로 구성된 하이퍼파라미터를 parameter_grid(common 항목 참조) 함수를 통해 하이퍼파라미터 조합을 생성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_hyperparameter(model: str):\n",
    "    if model == 'logistic':\n",
    "        params_dict = {\n",
    "            'C': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, \n",
    "                  1, 2, 3, 4, 5, 7, 9, 11, 15, 20, 25, 30, 35, 40, 50, 100],\n",
    "            'penalty': ['l1', 'l2'],\n",
    "            'solver': ['liblinear', 'saga']\n",
    "        }\n",
    "        \n",
    "    elif model == 'dt':\n",
    "        params_dict = {\n",
    "            'criterion': ['gini', 'entropy'],\n",
    "            'max_depth': [None, 1, 2, 3, 4, 5],\n",
    "            'min_samples_split': [2, 3, 4],\n",
    "            'min_samples_leaf': [1, 2, 3]\n",
    "        }\n",
    "    \n",
    "    elif model == 'rf':\n",
    "        params_dict = {\n",
    "            'n_estimators': [3, 5, 10, 15, 20, 30, 50, 90, 95, \n",
    "                            100, 125, 130, 150],\n",
    "            'criterion': ['gini'],\n",
    "            'min_samples_split': [2, 4],\n",
    "            'min_samples_leaf': [1, 3],\n",
    "            'max_features': ['sqrt', 'log2']\n",
    "        }\n",
    "    \n",
    "    elif model == 'gbt':\n",
    "        params_dict = {\n",
    "            'learning_rate': [0.001, 0.005, 0.01, 0.05, 0.1],\n",
    "            'n_estimators': [5, 10, 50, 100, 130],\n",
    "            'max_depth': [1, 2, 3, 4]\n",
    "        }\n",
    "    \n",
    "    elif model == 'xgb':\n",
    "        params_dict = {\n",
    "        'min_child_weight': [1, 2, 3, 5],\n",
    "            'max_depth': [3, 6, 9],\n",
    "            'gamma': np.linspace(0, 3, 10),\n",
    "            # 'objective': ['multi:softmax'],\n",
    "            'booster': ['gbtree']\n",
    "        }\n",
    "    \n",
    "    elif model == 'lgb':\n",
    "        params_dict = {\n",
    "            # 'objective': ['multiclass'],\n",
    "            'num_leaves': [15, 21, 27, 31, 33],\n",
    "            'max_depth': [-1, 2],\n",
    "            'n_estimators': [5, 10, 50, 100, 130],\n",
    "            'min_child_samples': [10, 20, 25, 30]\n",
    "        }\n",
    "    \n",
    "    elif model == 'lda':\n",
    "        params_dict1 = {\n",
    "            'solver': ['lsqr', 'eigen'],\n",
    "            'shrinkage': np.logspace(-3, 0, 30)\n",
    "        }\n",
    "        params_dict2 = {\n",
    "            'solver': ['svd'],\n",
    "            'tol': np.logspace(-5, -3, 20)\n",
    "        }\n",
    "    \n",
    "    elif model == 'qda':\n",
    "        params_dict = {\n",
    "            'reg_param': np.append(np.array([0]), np.logspace(-5, 0, 10)),\n",
    "            'tol': np.logspace(-5, -3, 10)\n",
    "        }\n",
    "    \n",
    "    elif model == 'plsda':\n",
    "        params_dict = {\n",
    "            'n_components': [1, 2, 3],\n",
    "            'max_iter': [300, 500, 1000],\n",
    "            'tol': np.logspace(-7, -5, 10)\n",
    "        }\n",
    "        \n",
    "    elif model == 'mlp':\n",
    "        params_dict = {\n",
    "            'hidden_layer_sizes': [(50), (100, 50, 10), (100, 70, 50, 30, 10)],\n",
    "            'activation': ['relu', 'tanh'],\n",
    "            'solver': ['adam', 'sgd'],\n",
    "            'alpha': [0.0001, 0.001],\n",
    "            'learning_rate_init': [0.001, 0.01, 0.1],\n",
    "            'max_iter': [50, 100, 200]\n",
    "        }\n",
    "    \n",
    "    #\n",
    "    if model == 'lda':\n",
    "        params = parameter_grid(params_dict1)\n",
    "        params.extend(parameter_grid(params_dict2))\n",
    "    else:\n",
    "        params = parameter_grid(params_dict)\n",
    "    \n",
    "    return params"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
